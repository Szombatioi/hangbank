"use client";
//TODO: handle space key to go next block
import {
  ZoomOut,
  RestartAlt,
  Pause,
  PlayArrow,
  ZoomIn,
  Mic,
  Stop,
  SettingsVoice,
} from "@mui/icons-material";
import { Box, IconButton, Paper, Tooltip, Typography } from "@mui/material";
import { useEffect, useRef, useState } from "react";
import { useTranslation } from "react-i18next";
import { start } from "repl";
import WaveSurfer from "wavesurfer.js";

interface RecorderProps {
  selectedDeviceId: string | null;
  sampleRate: number;
  save_freq_ms: number;
  useTranscript: boolean;
  onAudioUpdate?: (url: string) => void;
  onRecordingStop: (finalBlob: Blob, transcription?: string) => void;
  onSpacePress?: (blob: Blob) => void;
  onTranscriptUpdate?: (text: string) => void;
  language: string;
}

export default function Recorder({
  selectedDeviceId,
  sampleRate,
  save_freq_ms,
  useTranscript,
  onAudioUpdate,
  onRecordingStop,
  onSpacePress,
  language,
  onTranscriptUpdate,
}: RecorderProps) {
  const { t } = useTranslation("common");

  const [stream, setStream] = useState<MediaStream | null>(null); //represents the microphone
  const [recorder, setRecorder] = useState<MediaRecorder | null>(null); //represents the recording process
  const recorderRef = useRef<MediaRecorder | null>(null);
  const [isRecording, setIsRecording] = useState<boolean>(false); //whether we are recording or not
  const [isPaused, setIsPaused] = useState<boolean>(false); //whether we are paused or not
  const isRecordingRef = useRef(isRecording);
  const isPausedRef = useRef(isPaused);
  const [audioUrl, setAudioUrl] = useState<string | null>(null); //the recorded audio URL to represent as waveform and to play
  const audioUrlRef = useRef<string | null>(null);
  const audioChunksRef = useRef<Blob[]>([]); //to store the recorded audio chunks

  //WebSpeechAPI transcript
  const [wsi_transcript, setWsiTranscript] = useState(""); //Wsi transcript = Web Speech API transcript, which can restart if we are silent for too long
  const wsiTranscriptRef = useRef(wsi_transcript);
  const [transcript, setTranscript] = useState("");
  const recognitionRef = useRef<SpeechRecognition | null>(null);

  // Minden renderel√©skor szinkroniz√°lja a Ref-eket a State-tel
  useEffect(() => {
    isRecordingRef.current = isRecording;
    isPausedRef.current = isPaused;
    wsiTranscriptRef.current = wsi_transcript;
    audioUrlRef.current = audioUrl; // üí° SZINKRONIZ√ÅL√ÅS HOZZ√ÅADVA
  }, [isRecording, isPaused, wsi_transcript, audioUrl]);

  useEffect(() => {
    recorderRef.current = recorder;
  }, [recorder]);

  //Init SpeechRecognition
  useEffect(() => {
    if (!useTranscript) return;
    // console.log("Transcribe is enabled");

    if (typeof window === "undefined") return; // ensure client-side
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error("SpeechRecognition not supported in this browser.");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true; // keep listening
    recognition.interimResults = true; // get partial results
    recognition.lang = language;

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      //Only transcribe if recording and not paused
      // console.log(isRecordingRef.current, isPausedRef.current);
      if (!isRecordingRef.current || isPausedRef.current) return;

      let final = ""; // Lez√°rt, v√©gleges sz√∂veg az aktu√°lis esem√©nyben
      let interim = ""; // Ideiglenes, m√©g v√°ltoz√≥ sz√∂veg

      for (let i = event.resultIndex; i < event.results.length; ++i) {
        const transcriptPart = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          final += transcriptPart;
        } else {
          interim += transcriptPart;
        }
      }

      // 1. Friss√≠tj√ºk a F≈ê (lez√°rt) transcript √°llapotot:
      if (final.length > 0) {
        // Hozz√°adjuk a v√©gleges sz√∂veget a kor√°bbi v√©gleges sz√∂veghez
        setTranscript((prev) => (prev.trim() + " " + final.trim()).trim());
      }

      // 2. A Webspeech API √°ltal adott teljes (lez√°rt + ideiglenes) sz√∂veget t√°roljuk a WSI state-ben
      // Ezt jelen√≠theted meg a dobozban, mint a pillanatnyi sz√∂veget.
      // Mivel az event.results m√°r tartalmazza az interim r√©szt, csak ezt kell be√°ll√≠tani:
      setWsiTranscript(interim);
    };

    recognition.onerror = (err: any) =>
      console.error("Recognition error:", err);

    recognitionRef.current = recognition;
  }, []);

  useEffect(() => {
    // console.log("onTranscriptUpdate: ", transcript, wsiTranscriptRef.current)
    if (onTranscriptUpdate) {
      onTranscriptUpdate(transcript + wsi_transcript);
    }
  }, [transcript, wsi_transcript]);

  const startTranscribe = () => {
    if (useTranscript && recognitionRef.current) {
      // üí° JAV√çT√ÅS: A start() h√≠v√°st try...catch blokkba tessz√ºk
      try {
        recognitionRef.current.start();
        // console.log("Transcribe started in startTranscribe function");
      } catch (error: any) {
        // Ha m√°r fut, az "InvalidStateError" hib√°t kapjuk, 
        // amit egyszer≈±en figyelmen k√≠v√ºl hagyunk.
        if (error.name === 'InvalidStateError') {
          console.warn("SpeechRecognition already started, ignoring redundant call.");
        } else {
          // M√°s hiba eset√©n jelezz√ºk
          console.error("SpeechRecognition start error:", error);
        }
      }
    }
  };
  const stopTranscribe = () => {
    if (useTranscript) recognitionRef.current?.stop();
  };

  const waveformRef = useRef<HTMLDivElement | null>(null);
  const waveSurferRef = useRef<WaveSurfer | null>(null);
  const [isPlayingAudio, setIsPlayingAudio] = useState<boolean>(false);
  const toggleAudioPlay = () => {
    setIsPlayingAudio(!isPlayingAudio);
  };

  //Start/Stop playing audio with WaveSurfer when isPlayingAudio changes
  useEffect(() => {
    const ws = waveSurferRef.current;
    if (!ws) return;

    if (isPlayingAudio) {
      ws.play();
    } else {
      ws.pause();
    }
  }, [isPlayingAudio]);

  //TODO: make this number adjusted based on the audio length
  const [minPxPerSec, setMinPxPerSec] = useState<number>(100); //1-1000?
  const handleSliderChange = (event: Event, newValue: number) => {
    setMinPxPerSec(newValue);
  };

  //Update zoom level when minPxPerSec changes
  useEffect(() => {
    if (waveSurferRef.current) {
      waveSurferRef.current.zoom(minPxPerSec);
    }
  }, [minPxPerSec]);

  //Initialize WaveSurfer
  useEffect(() => {
    if (!waveformRef.current) return;

    // Create wavesurfer only once
    waveSurferRef.current = WaveSurfer.create({
      container: waveformRef.current,
      waveColor: "#4F4A85",
      progressColor: "#383351",
      cursorColor: "#A6A3FF",
      barWidth: 2,
      height: 100,
      url: "",
      fillParent: false,
      minPxPerSec: minPxPerSec,
      autoScroll: true,
      autoCenter: true,
    });
    waveSurferRef.current.on("click", () => {
      setIsPlayingAudio(true);
      waveSurferRef!.current!.play();
    });

    waveSurferRef.current.on("finish", () => {
      setIsPlayingAudio(false);
    });

    // console.log(isPlayingAudio);

    return () => {
      waveSurferRef.current?.destroy();
      waveSurferRef.current = null;
    };
  }, []);

  //Update waveform if there is a new audio URL (every 1 sec)
  useEffect(() => {
    console.log("New audio url")
    const ws = waveSurferRef.current;
    if (!ws) return;
    console.log("Ws is not null")
    if (audioUrl) {
      console.log("AudioURL is not null")
      // T√∂r√∂lj√ºk a r√©gi URL-t, miel≈ëtt √∫jat t√∂lt√ºnk be
      if (ws.isPlaying()) {
        ws.pause();
        setIsPlayingAudio(false);
      }

      // Bet√∂ltj√ºk az √∫jonnan gener√°lt Blob URL-t
      console.log("Loading new audio URL");
      ws.load(audioUrl).catch((err) =>
        console.error("WaveSurfer hiba a bet√∂lt√©skor:", err)
      );
      console.log("Loaded");

      // Nagyon fontos: felszabad√≠tjuk a r√©gi Blob URL-t
      // (B√°r a setAudioUrl az √∫j URL-t kapja, a r√©gi m√°r nem kell, ha nem mentj√ºk el)
      // Ezt a cleanup-ban c√©lszer≈±bb megoldani.
    } else if (!audioUrl) {
      console.log("AudioURL is null")
      ws.empty();
    }
  }, [audioUrl]);

  //Function to get microphone stream and setup recorder
  const getStreamAndSetupRecorder = async () => {
    //Stream already exists
    if (stream) {
      return;
    }

    try {
      const constraints = {
        audio: {
          deviceId: selectedDeviceId ? { exact: selectedDeviceId } : undefined,
          sampleRate: { ideal: sampleRate },
        },
      };

      const newStream = await navigator.mediaDevices.getUserMedia(constraints);
      setStream(newStream);

      const newRecorder = new MediaRecorder(newStream, {
        mimeType: "audio/webm; codecs=opus",
        bitsPerSecond: 512000, //512 kbps
      });
      setRecorder(newRecorder);

      // A 'dataavailable' esem√©nykezel≈ë be√°ll√≠t√°sa a folyamatos Blob ment√©shez
      newRecorder.ondataavailable = (event) => {
        // Csak akkor dolgozzuk fel, ha van adat
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);

          //Set the audio URL for playback and visualization
          const fullBlob = new Blob(audioChunksRef.current, {
            type: "audio/webm",
          });

          const newUrl = URL.createObjectURL(fullBlob);
          setAudioUrl((prevAudioUrl) => {
            if (prevAudioUrl) {
              URL.revokeObjectURL(prevAudioUrl);
            }
            return newUrl; // Visszaadjuk az √∫j URL-t
          });

          if (onAudioUpdate) {
            onAudioUpdate(newUrl);
          }
        }
      };

      newRecorder.onstop = () => {
        newStream?.getTracks().forEach((track) => track.stop());

        //Final blob
        if (audioChunksRef.current.length > 0) {
          const finalBlob = new Blob(audioChunksRef.current, {
            type: "audio/webm",
          });

          onRecordingStop(finalBlob);
          // console.log("Felv√©tel sikeresen lez√°rva. V√©gleges Blob √°tadva.");
        } else {
          // Ez a helyzet akkor √°llhat el≈ë, ha t√∫l gyorsan nyomt√°k meg a stop-ot
          // console.warn("R√∂gz√≠t√©s le√°ll√≠tva, de nincs r√∂gz√≠tett adat.");
        }

        if (audioUrlRef.current) {
          URL.revokeObjectURL(audioUrlRef.current);
          setAudioUrl(null);
        }

        // √Ållapotok null√°z√°sa
        audioChunksRef.current = [];
        setStream(null);
        setRecorder(null);
        // Az isRecording √©s isPaused m√°r a stopRecording-ban friss√ºlt
      };

      //We start the recording here because setXYZ is async and we want to start right away
      newRecorder.start(save_freq_ms);
      // console.log("Recording started");
      setIsRecording(true);
      setIsPaused(false);
    } catch (err) {
      alert(t("no_mic_access"));
      throw new Error();
    }
  };

  const getAudioBlobAndUrl = () => {
    const currentFullBlob = new Blob(audioChunksRef.current, {
      type: "audio/webm",
    });

    if (audioUrl) {
      URL.revokeObjectURL(audioUrl);
    }

    const newUrl = URL.createObjectURL(currentFullBlob);
    setAudioUrl(newUrl);
  };

  const startRecording = async () => {
    if (isRecording) return;

    audioChunksRef.current = [];
    setAudioUrl(null);
    setTranscript("");
    setWsiTranscript("");

    try {
      await getStreamAndSetupRecorder();
      if(transcript.length === 0)
        startTranscribe();
      // if (recorder && stream) {
      //   recorder!.start(save_freq_ms);
      //   setIsPaused(false);
      //   setIsRecording(true);
      // }
    } catch (err) {
      alert(t("no_mic_access"));
      alert(err);
    }
  };

  const pauseRecording = () => {
    if (recorder && isRecording && !isPaused) {
      recorder!.pause();
      setIsPaused(true);
      // setIsRecording(false);
      getAudioBlobAndUrl(); //Give URL back to caller on pause

      //Give blob to the caller side even on pause!
      const finalBlob = new Blob(audioChunksRef.current, {
        type: "audio/webm",
      });
      if (onAudioUpdate) onAudioUpdate(audioUrl!);
      if (onRecordingStop) onRecordingStop(finalBlob);
    }
  };

  const resumeRecording = () => {
    if (recorder && isRecording && isPaused) {
      recorder.resume();
      setIsPaused(false);
      // console.log("Felv√©tel folytatva.");
    }
  };

  const stopRecording = () => {
    const currentRecorder = recorder;
    const currentStream = stream;
    const currentAudioUrl = audioUrl;

    if (currentRecorder && (isRecording || isPaused)) {
      // √Åll√≠tsuk le a MediaRecorder-t. Ezzel kibocs√°tja az utols√≥ ondataavailable-t, majd az onstop-ot.
      currentRecorder.stop();

      // Friss√≠tj√ºk a legfontosabb √°llapotot, hogy a gombok letilt√≥djanak
      setIsRecording(false);
      setIsPaused(false);
      
      setAudioUrl(null);
      // Felszabad√≠tjuk a mem√≥ri√°t, ami a lej√°tsz√°shoz kellett
      // if (currentAudioUrl) {
      //   URL.revokeObjectURL(currentAudioUrl);
      // }
      stopTranscribe();
      // console.log("Felv√©tel le√°ll√≠t√°si k√©r√©se elk√ºldve.");
    }
  };

  //Handle Spacebar press to go to next block
  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      // Csak akkor fusson, ha van "onSpacePress" prop, r√∂gz√≠t√ºnk √©s nem vagyunk sz√ºneteltetve
      if (
        onSpacePress &&
        event.code === "Space" &&
        isRecordingRef.current &&
        !isPausedRef.current
      ) {
        event.preventDefault(); // Megakad√°lyozza az oldal g√∂rget√©s√©t
        
        console.log("Spacebar: Blokkv√°lt√°s. Jelenlegi Blob ment√©se √©s null√°z√°sa.");

        // 1. Elk√ºldj√ºk az EDDIGI blob-ot a h√≠v√≥ oldalnak (ahogy eddig is)
        if (onSpacePress) {
          onSpacePress(
            new Blob(audioChunksRef.current, {
              type: "audio/webm",
            })
          );
          console.log("El≈ëz≈ë blokk Blob-ja elk√ºldve.");
        }

        // 2. üí° NULL√ÅZZUK A BLOB-T√ÅROL√ìT (a k√©r√©sed szerint)
        // A MediaRecorder tov√°bb fut, √©s a k√∂vetkez≈ë 'ondataavailable' 
        // esem√©ny m√°r ebbe az √ºres t√∂mbbe fogja helyezni az adatot.
        audioChunksRef.current = [];

        // 3. NULL√ÅZZUK a vizualiz√°ci√≥t √©s a transzkripci√≥t
        // Felszabad√≠tjuk a r√©gi URL-t a mem√≥riasziv√°rg√°s elker√ºl√©se √©rdek√©ben
        // A 'ref'-et haszn√°ljuk, hogy biztosan a legut√≥bbi URL-t kapjuk meg
        if (audioUrlRef.current) {
          URL.revokeObjectURL(audioUrlRef.current);
        }
        
        // Ez triggereli a WaveSurfer useEffect-et, ami megh√≠vja a `ws.empty()`-t
        setAudioUrl(null); 
        
        // Transzkripci√≥ null√°z√°sa az √∫j blokkhoz
        setTranscript("");
        setWsiTranscript("");

      }
    };

    window.addEventListener("keydown", handleKeyDown);

    return () => {
      window.removeEventListener("keydown", handleKeyDown);
    };
  }, [onSpacePress]); // A f√ºgg≈ës√©gi t√∂mb√∂t friss√≠tettem [onSpacePress]-re

  return (
    <div
      style={{
        justifySelf: "center",
        // border: "1px solid red",
        width: "75%",
        overflowX: "auto",
        overflowY: "hidden",
        whiteSpace: "nowrap",
        marginTop: 8,
        marginBottom: 8,
      }}
    >
      <Paper
        style={{
          display: "flex",
          flexDirection: "column",
          justifyContent: "center",
          padding: 4,
          boxShadow: "none",
          border: "2px solid #ccc",
        }}
        elevation={4}
      >
        {/* Waveform */}
        <div id="waveform" ref={waveformRef} />
        {/* Recording buttons */}
        <div style={{ display: "flex", justifyContent: "center", gap: 4 }}>
          <IconButton
            onClick={() => {
              if (!isRecording) {
                startRecording();
              } else {
                if (isPaused) {
                  resumeRecording();
                } else {
                  pauseRecording();
                }
              }
            }}
            size="medium"
            sx={{ boxShadow: "0px 0px 10px rgba(0, 0, 0, 0.2)" }}
          >
            {isRecording ? (
              isPaused ? (
                <>
                  <SettingsVoice />
                </>
              ) : (
                <>
                  <Pause />
                </>
              )
            ) : (
              <>
                <Mic />
              </>
            )}
          </IconButton>

          {isRecording && (
            <>
              <IconButton
                onClick={() => {
                  stopRecording();
                }}
                size="medium"
                sx={{ boxShadow: "0px 0px 10px rgba(0, 0, 0, 0.2)" }}
              >
                <Stop />
              </IconButton>
            </>
          )}

          {audioUrl && !isRecording && (
            <>
              <IconButton
                onClick={() => {
                  toggleAudioPlay();
                }}
                size="medium"
                sx={{ boxShadow: "0px 0px 10px rgba(0, 0, 0, 0.2)" }}
              >
                {isPlayingAudio ? <Pause /> : <PlayArrow />}
              </IconButton>
            </>
          )}
        </div>
      </Paper>

      {/* Transcript */}
      {useTranscript && (
        <>
          <div style={{ marginTop: 8, marginBottom: 8 }}>
            <Box
              sx={{
                justifySelf: "center",
                // border: "1px solid red",
                width: "100%",
                minHeight: 256,
                maxHeight: 512,
                display: "flex",
                justifyContent: "center",
                textWrap: "wrap",
              }}
            >
              <Paper
                sx={{
                  display: "flex",
                  width: "100%",
                  flexDirection: "column",
                  padding: 2,
                  boxShadow: "none",
                  border: "2px solid #ccc",
                  justifyContent: "space-between",
                  alignItems: "space-between",
                  textWrap: "wrap",
                }}
              >
                <Typography>{transcript + wsi_transcript}</Typography>

                <Typography
                  align="center"
                  sx={{
                    marginBlock: -2,
                    color: "#ccc",
                  }}
                >
                  {t("transcription")}
                </Typography>
              </Paper>
            </Box>
          </div>
        </>
      )}
    </div>
  );
}
